{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3812a71",
   "metadata": {},
   "source": [
    "# SETUP AND USING TENSORFLOW GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3538424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "import shutil\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "\n",
    "\n",
    "# import DCGAN as gan\n",
    "\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, Activation, BatchNormalization, Lambda, MaxPooling2D,SpatialDropout2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras_visualizer import visualizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df988991",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6dffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f1bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = './PokemonData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71228cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_folder():\n",
    "    os.system(\"mkdir train\")\n",
    "    os.system(\"mkdir test\")\n",
    "    os.system(\"mkdir val\")\n",
    "    \n",
    "def clean_images():\n",
    "    os.system(\"rm -r train/* test/* val/*\")\n",
    "    os.system(\"find train/ -name '*.*' -type f -delete\")\n",
    "    os.system(\"find val/ -name '*.*' -type f -delete\")\n",
    "    os.system(\"find test/ -name '*.*' -type f -delete\")\n",
    "\n",
    "def data_extractor(class_pct, train_pct, val_pct, test_pct):\n",
    "    dirs = os.listdir(IMG_DIR)\n",
    "    sub_classes = random.sample(dirs,int(len(dirs)*class_pct))\n",
    "\n",
    "    for cla in sub_classes:\n",
    "        os.system('mkdir \"train/'+cla+'\"')\n",
    "        os.system('mkdir \"test/'+cla+'\"')\n",
    "        os.system('mkdir \"val/'+cla+'\"')\n",
    "        os.system(\"find train/\"+cla+\" -name '*.*' -type f -delete\")\n",
    "        os.system(\"find val/\"+cla+\" -name '*.*' -type f -delete\")\n",
    "        os.system(\"find test/\"+cla+\" -name '*.*' -type f -delete\")\n",
    "\n",
    "        temp_files = os.listdir(os.path.join(IMG_DIR,cla))\n",
    "\n",
    "        files = [f for f in temp_files if isfile(os.path.join(IMG_DIR, cla,f))]\n",
    "        \n",
    "        train_files = random.sample(files,int(len(files)*(train_pct+val_pct+test_pct)))\n",
    "        val_files = random.sample(train_files,int(len(files)*(val_pct+test_pct)))\n",
    "        test_files = random.sample(val_files,int(len(files)*test_pct))\n",
    "        train_files = [x for x in train_files if x not in val_files]\n",
    "        val_files = [x for x in val_files if x not in test_files]\n",
    "        \n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(IMG_DIR,cla,file), os.path.join('train',cla,file))\n",
    "        for file in val_files:\n",
    "            shutil.copy(os.path.join(IMG_DIR,cla,file), os.path.join('val',cla,file))\n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(IMG_DIR,cla,file), os.path.join('test',cla,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c69717",
   "metadata": {},
   "outputs": [],
   "source": [
    "SampleSize= 1\n",
    "TrainSize = 0.7\n",
    "TestSize = 0.2\n",
    "ValSize= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89196f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_extractor(SampleSize, TrainSize, ValSize, TestSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82d006",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0428c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.resize(image, [192, 192])\n",
    "  image /= 255.0  # normalize to [0,1] range\n",
    "\n",
    "  return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "  image = tf.io.read_file(path)\n",
    "  return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beea25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('./train')\n",
    "Test_root = Path('./test')\n",
    "val_root = Path('./val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ac25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "  return load_and_preprocess_image(path), label\n",
    "\n",
    "def create_dataset (root):\n",
    "    all_image_paths = list(root.glob('*/*.jpg'))\n",
    "    all_image_paths = [str(path) for path in all_image_paths]\n",
    "    random.shuffle(all_image_paths)\n",
    "\n",
    "    image_count = len(all_image_paths)\n",
    "\n",
    "    print(\"The image count is %d\" % image_count)\n",
    "    label_names = sorted(item.name for item in root.glob('*/') if item.is_dir())\n",
    "\n",
    "    label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
    "\n",
    "    all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                        for path in all_image_paths]\n",
    "\n",
    "    print(\"First 10 labels indices: \", all_image_labels[:10])\n",
    "\n",
    "    steps_per_epoch=tf.math.ceil(len(all_image_paths)/BATCH_SIZE).numpy()\n",
    "    print(steps_per_epoch)\n",
    "\n",
    "    paths_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "    image_ds = paths_ds.map(load_and_preprocess_image)\n",
    "\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64))\n",
    "    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "    image_label_ds = ds.map(load_and_preprocess_from_path_label)\n",
    "    \n",
    "\n",
    "    ds = image_label_ds.shuffle(buffer_size=image_count)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2063cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image count is 4733\n",
      "First 10 labels indices:  [4, 50, 137, 75, 39, 103, 137, 41, 9, 146]\n",
      "148.0\n",
      "The image count is 653\n",
      "First 10 labels indices:  [115, 129, 110, 50, 134, 76, 55, 84, 121, 105]\n",
      "21.0\n"
     ]
    }
   ],
   "source": [
    "train_ds = create_dataset(data_root)\n",
    "val_ds = create_dataset(val_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3da2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "image_shape = (192,192,3)\n",
    "ClassNum = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ad1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=(3,3),padding='same',input_shape=image_shape,activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same',input_shape=image_shape,activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same',input_shape=image_shape,activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "\n",
    "    model.add(Dense(150,activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114dfa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_decay(initial_rate, decay_step, decay_rate):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=(3,3),padding='same',input_shape=image_shape,activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same',input_shape=image_shape,activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same',input_shape=image_shape,activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "\n",
    "    model.add(Dense(150,activation='softmax'))\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_rate,\n",
    "    decay_steps=decay_step,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5877254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compared_decay_rate():\n",
    "    initials = [0.001,0.0001]\n",
    "    steps = [10000,100000]\n",
    "    rates = [0.96]\n",
    "    historys =[]\n",
    "    for initial in initials:\n",
    "        for step in steps:\n",
    "            for rate in rates:\n",
    "                model = build_model_with_decay(initial,step,rate)\n",
    "                history = model.fit(train_ds, epochs=EPOCH, validation_data = val_ds)\n",
    "                historys.append(history)\n",
    "                loss_history = pd.DataFrame(history.history)\n",
    "                plt.plot(loss_history['val_accuracy'],label=\"initial: \"+str(initial)+ \" steps: \"+str(step)+ \" rate: \"+str(rate))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(\"compared_decay_rate_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".png\")\n",
    "    return historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a6e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 1664, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py\", line 4994, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 150) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12356/2409571829.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.96\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model_with_decay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./historys/compare_lr_decay_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 1664, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\15197\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py\", line 4994, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 150) are incompatible\n"
     ]
    }
   ],
   "source": [
    "initial = 0.001,\n",
    "step = 100\n",
    "rate = 0.96\n",
    "model = build_model_with_decay(initial,step,rate)\n",
    "history = model.fit(train_ds, epochs=EPOCH, validation_data = val_ds)\n",
    "json.dump(history.history,open(\"./historys/compare_lr_decay_\"+str(initial)+\"_\"+str(step)+\"_\"+str(rate),'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c3264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f941d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdeb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
